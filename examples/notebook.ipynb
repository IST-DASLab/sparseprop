{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparseProp Usage Guide\n",
    "\n",
    "This notebook serves as a guide on how to effectively utilize SparseProp. You'll find detailed steps on how to take advantage of SparseProp for both individual layers, as well as the entire network to accelerate the backpropagation process.\n",
    "\n",
    "As an introduction, SparseProp provides a low-level CPU implementation of backpropagation, where the weights of a layer are unstructured sparse. More specifically, if we have a sparse fully connected or convolution layer, SparseProp is capable of speeding up the backpropagation process on CPU. We further integrate SparseProp with the PyTorch framework, providing the *SparseLinear* and *SparseConv2d* modules as drop-in replacements for PyTorch's *Linear* and *Conv2d* modules, respectively. Further details of our algorithms can be found [the our paper](https://arxiv.org/abs/2302.04852).\n",
    "\n",
    "Now, let's get started! We first set the random seeds to get consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All seeds were set to 10.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 10\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "print(f\"All seeds were set to {seed}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Layer\n",
    "\n",
    "Let's say we have a *Linear* module, which is 95% sparse. For the sake of argument, let's actually create such module. We assume that the input and output dimensions are both 512, but any other dimensions work just as fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our module's spasity is now 0.95.\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "\n",
    "linear = Linear(512, 512) # 512 input channels, and 512 output channels\n",
    "\n",
    "# prune the module randomly to 95% unstructred sparsity\n",
    "with torch.no_grad():\n",
    "\n",
    "    # generate a random mask with roughly 95% sparsity\n",
    "    mask = torch.rand_like(linear.weight) > 0.95\n",
    "\n",
    "    # apply the mask to the module\n",
    "    linear.weight.mul_(mask.float())\n",
    "\n",
    "print(f\"Our module's spasity is now {(linear.weight == 0).float().mean().item():.2f}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we actually have a 95% sparse module, called `linear`. Let's see how long forward and backward steps take on this module. Assuming the batch size is 256, we generate a synthetic batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(256, 512) # batch_size x input_dimension\n",
    "\n",
    "# the following two lines tell PyTorch to store the gradients for the input tensor\n",
    "X.requires_grad_()\n",
    "X.retain_grad()\n",
    "\n",
    "y = torch.randn(256, 512) # batch_size x output_dimension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we  measure the time required for the forward and backward steps of the `linear` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The forward pass took 0.012 seconds.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# time the forward step\n",
    "start = time.time()\n",
    "O = linear(X)\n",
    "pytorch_forward_time = time.time() - start\n",
    "f\"The forward pass took {pytorch_forward_time:.3f} seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The backward pass took 0.027 seconds.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the mse loss\n",
    "L = torch.mean((y - O) ** 2)\n",
    "\n",
    "# time the backward step\n",
    "start = time.time()\n",
    "L.backward()\n",
    "pytorch_backward_time = time.time() - start\n",
    "f\"The backward pass took {pytorch_backward_time:.3f} seconds.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the forward and backward passes took 12 and 26 milliseconds, respectively. But notice we haven't exploited *SparseProp*'s implementations yet. Let's see how much speedup we can get if we utilize SparseProp.\n",
    "\n",
    "To do so, we only need one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseLinear([512, 512], sp=0.95, nnz=13190)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sparseprop.modules import SparseLinear\n",
    "\n",
    "# this line will convert your pytorch module to a sparseprop module\n",
    "sparse_linear = SparseLinear.from_dense(linear)\n",
    "\n",
    "sparse_linear"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a *SparseProp* module, which will benefit from our high performance sparse implementations. Let's again compute the forward and backward times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The forward pass took 0.003 seconds.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time the forward step\n",
    "start = time.time()\n",
    "O = sparse_linear(X)\n",
    "sparseprop_forward_time = time.time() - start\n",
    "f\"The forward pass took {sparseprop_forward_time:.3f} seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The backward pass took 0.004 seconds.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the mse loss\n",
    "L = torch.mean((y - O) ** 2)\n",
    "\n",
    "# time the backward step\n",
    "start = time.time()\n",
    "L.backward()\n",
    "sparseprop_backward_time = time.time() - start\n",
    "f\"The backward pass took {sparseprop_backward_time:.3f} seconds.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you should vitness a significant speedup in *SparseProp* with respect to PyTorch's implementations. Run the following cell to compare the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward speedup: 4.67x\n",
      "Backward speedup: 6.26x\n"
     ]
    }
   ],
   "source": [
    "print(f\"Forward speedup: {pytorch_forward_time / sparseprop_forward_time:.2f}x\")\n",
    "print(f\"Backward speedup: {pytorch_backward_time / sparseprop_backward_time:.2f}x\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a `Conv2d` module instead of a `Linear` one, you can again use *SparseProp* to gain speedups. The interface is exactly the same with only one differnce. If your module is called `conv`, you can do:\n",
    "\n",
    "```\n",
    "from sparseprop.modules import SparseConv2d\n",
    "\n",
    "sparse_conv = SparseConv2d.from_dense(conv, vectorizing_over_on=False)\n",
    "```\n",
    "\n",
    "The only difference with the *Linear* case is that there is an additional boolean argument `vectorizing_over_on`. As described in the paper, we have two implementations for the convolution case, one performing the vectorization over the bactch size, and the other over the output dimension. Using this argument you can specify which one of the two implementations to use. A quick rule of thumb is that if the input width and height are small (e.g., less than 32) then `vectorizing_over_on=False` is faster.\n",
    "\n",
    "Alternatively, the `sparsify_conv2d_auto` method can automatically determine the correct value of `vectorizing_over_on`.\n",
    "\n",
    "```\n",
    "from sparseprop.modules import sparsify_conv2d_auto\n",
    "\n",
    "sparse_conv = sparsify_conv2d_auto(conv, input_shape, verbose=True)\n",
    "```\n",
    "\n",
    "Notice that you will need to feed the `input_shape` to this method, which should look something like (`batch_size`, `input_channels`, `input_height`, `input_width`). This method will create two sparse modules, one with `vectorizing_over_on=False` and the other one with `vectorizing_over_on=True`, run a randomly generated batch through both, and return the faster module based on forward+backward time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nm)",
   "language": "python",
   "name": "nm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
