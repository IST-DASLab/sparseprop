{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparseProp Usage Guide\n",
    "\n",
    "This notebook serves as a guide on how to effectively utilize SparseProp. You'll find detailed steps on how to take advantage of SparseProp for both individual layers, as well as the entire network to accelerate the backpropagation process.\n",
    "\n",
    "As an introduction, SparseProp provides a low-level CPU implementation of backpropagation, where the weights of a layer are unstructured sparse. More specifically, if we have a sparse fully connected or convolution layer, SparseProp is capable of speeding up the backpropagation process on CPU. We further integrate SparseProp with the PyTorch framework, providing the *SparseLinear* and *SparseConv2d* modules as drop-in replacements for PyTorch's *Linear* and *Conv2d* modules, respectively. Further details of our algorithms can be found in [our paper](https://arxiv.org/abs/2302.04852).\n",
    "\n",
    "If you haven't already installed *SparseProp*, make sure you have PyTorch installed, and then simply run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sparseprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get started! Here we only consider the case where only a single thread is being used for doing the computations. Run the following cell to limit both *PyTorch* and *SparseProp* to a single thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sparseprop\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "sparseprop.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's set the random seeds to get consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All seeds were set to 10.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed = 10\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "f\"All seeds were set to {seed}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Layer\n",
    "\n",
    "Let's say we have a *Linear* module, which is 98% sparse. For the sake of argument, let's actually create such module. We assume that the input and output dimensions are 768 and 3072, respectively, but any other dimensions work just as fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Our module's spasity is now 0.98.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "\n",
    "linear = Linear(768, 3072) # input size of 768, and output size of 3072\n",
    "\n",
    "# prune the module randomly to 98% unstructred sparsity\n",
    "with torch.no_grad():\n",
    "\n",
    "    # generate a random mask with roughly 98% sparsity\n",
    "    mask = torch.rand_like(linear.weight) > 0.98\n",
    "\n",
    "    # apply the mask to the module\n",
    "    linear.weight.mul_(mask.float())\n",
    "\n",
    "f\"Our module's spasity is now {(linear.weight == 0).float().mean().item():.2f}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we actually have a 98% sparse module, called `linear`. Let's see how long forward and backward steps take on this module. Assuming the batch size is 2048, we generate a synthetic batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(2048, 768) # batch_size x input_dimension\n",
    "\n",
    "# the following two lines tell PyTorch to keep the gradients for the input tensor\n",
    "X.requires_grad_()\n",
    "X.retain_grad()\n",
    "\n",
    "y = torch.randn(2048, 3072) # batch_size x output_dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we  measure the time required for the forward and backward steps of the `linear` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The forward pass took 0.083 seconds.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# time the forward step\n",
    "start = time.time()\n",
    "O = linear(X)\n",
    "pytorch_forward_time = time.time() - start\n",
    "f\"The forward pass took {pytorch_forward_time:.3f} seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The backward pass took 0.196 seconds.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the mse loss\n",
    "L = torch.mean((y - O) ** 2)\n",
    "\n",
    "# time the backward step\n",
    "start = time.time()\n",
    "L.backward()\n",
    "pytorch_backward_time = time.time() - start\n",
    "f\"The backward pass took {pytorch_backward_time:.3f} seconds.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we haven't exploited *SparseProp*'s implementations yet. Let's see how much speedup we can get if we utilize SparseProp.\n",
    "\n",
    "To do so, we only need one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseLinear([3072, 768], sp=0.98, nnz=46972)\n"
     ]
    }
   ],
   "source": [
    "from sparseprop.modules import SparseLinear\n",
    "\n",
    "# this line will convert your pytorch module to a sparseprop module\n",
    "sparse_linear = SparseLinear.from_dense(linear)\n",
    "\n",
    "print(sparse_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a *SparseProp* module, let's again compute the forward and backward times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The forward pass took 0.038 seconds.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time the forward step\n",
    "start = time.time()\n",
    "O = sparse_linear(X)\n",
    "sparseprop_forward_time = time.time() - start\n",
    "f\"The forward pass took {sparseprop_forward_time:.3f} seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The backward pass took 0.084 seconds.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the mse loss\n",
    "L = torch.mean((y - O) ** 2)\n",
    "\n",
    "# time the backward step\n",
    "start = time.time()\n",
    "L.backward()\n",
    "sparseprop_backward_time = time.time() - start\n",
    "f\"The backward pass took {sparseprop_backward_time:.3f} seconds.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers you get will highly depend on your CPU architecture, but you should generally be able to see a non-trivial speedup with *SparseProp* with respect to PyTorch's implementations. Run the following cell to compare the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward speedup: 2.22x\n",
      "Backward speedup: 2.35x\n"
     ]
    }
   ],
   "source": [
    "print(f\"Forward speedup: {pytorch_forward_time / sparseprop_forward_time:.2f}x\")\n",
    "print(f\"Backward speedup: {pytorch_backward_time / sparseprop_backward_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a `Conv2d` module instead of a `Linear` one, you can again use *SparseProp* to gain speedups. The interface is exactly the same with only one differnce. If your module is called `conv`, you can do:\n",
    "\n",
    "```\n",
    "from sparseprop.modules import SparseConv2d\n",
    "\n",
    "sparse_conv = SparseConv2d.from_dense(conv, vectorizing_over_on=False)\n",
    "```\n",
    "\n",
    "The only difference with the *Linear* case is that there is an additional boolean argument `vectorizing_over_on`. As described in [the paper](https://arxiv.org/abs/2302.04852), we have two implementations for the convolution case, one performing the vectorization over the bactch size, and the other over the output dimension. Using this argument you can specify which one of the two implementations to use. A quick rule of thumb is that if the input width and height are small (e.g., less than 32) then `vectorizing_over_on=False` is faster.\n",
    "\n",
    "Alternatively, the `sparsify_conv2d_auto` method can automatically determine the correct value of `vectorizing_over_on`.\n",
    "\n",
    "```\n",
    "from sparseprop.modules import sparsify_conv2d_auto\n",
    "\n",
    "sparse_conv = sparsify_conv2d_auto(conv, input_shape, verbose=True)\n",
    "```\n",
    "\n",
    "Notice that you will need to feed the `input_shape` to this method, which should look something like (`batch_size`, `input_channels`, `input_height`, `input_width`). This method will create two sparse modules, one with `vectorizing_over_on=False` and the other one with `vectorizing_over_on=True`, run a randomly generated batch through both, and return the faster module based on forward+backward time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now assume you have a sparse network instead of just one layer. *SparseProp* offers tools that can seemlessly process your network object, and replace its layers with their corresponding sparse counterparts with only one or two lines of extra code. Let's go through an example of this case.\n",
    "\n",
    "Consider the scenario where you have a sparse model pre-trained on a large dataset (e.g., ImageNet). Let's say you want to fine-tune this sparse model on a smaller dataset (e.g., ImageNette), while keeping the sparsity mask fixed . This process is called *sparse transfer learning*.\n",
    "\n",
    "We have provided the checkpoint for a 95% uniform sparse ResNet18 model pre-trained on ImageNet at `models/resnet18_ac_dc_500_epochs_sp=0.95_uniform.pt`. Let's go ahead and load this model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by fixing the random seed so we get consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All seeds were set to 11.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the seed everywhere\n",
    "seed = 11\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "f\"All seeds were set to {seed}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a directory to store the log file and initialize a `Logger`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import Logger\n",
    "\n",
    "outdir = \"results-finetune-resnet18-imagenette/\"\n",
    "os.makedirs(outdir, exist_ok=False)\n",
    "logger = Logger(outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to load the checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "model = resnet18() # initialize the network\n",
    "ckpt = torch.load('models/resnet18_ac_dc_500_epochs_sp=0.95_uniform.pt', map_location='cpu') # read the checkpoint from the file\n",
    "model.load_state_dict(ckpt) # load the checkpoint into the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following block of code to print the sparsity level of each layer. Since the network is 95% uniformly sparse, we expect all the layers (except the first and last ones) to have a sparsity of exactly 95%.\n",
    "\n",
    "You will notice we have used the function `apply_to_all_modules_with_types(model, types, fn)`. This function iterates through the layers of `model`, and if the type of the layer is in the `types` list, it will apply the `fn` function on it and return the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity per layer:\n",
      "OrderedDict([   ('conv1', '0.000'),\n",
      "                ('layer1.0.conv1', '0.950'),\n",
      "                ('layer1.0.conv2', '0.950'),\n",
      "                ('layer1.1.conv1', '0.950'),\n",
      "                ('layer1.1.conv2', '0.950'),\n",
      "                ('layer2.0.conv1', '0.950'),\n",
      "                ('layer2.0.conv2', '0.950'),\n",
      "                ('layer2.0.downsample.0', '0.950'),\n",
      "                ('layer2.1.conv1', '0.950'),\n",
      "                ('layer2.1.conv2', '0.950'),\n",
      "                ('layer3.0.conv1', '0.950'),\n",
      "                ('layer3.0.conv2', '0.950'),\n",
      "                ('layer3.0.downsample.0', '0.950'),\n",
      "                ('layer3.1.conv1', '0.950'),\n",
      "                ('layer3.1.conv2', '0.950'),\n",
      "                ('layer4.0.conv1', '0.950'),\n",
      "                ('layer4.0.conv2', '0.950'),\n",
      "                ('layer4.0.downsample.0', '0.950'),\n",
      "                ('layer4.1.conv1', '0.950'),\n",
      "                ('layer4.1.conv2', '0.950'),\n",
      "                ('fc', '0.000')])\n"
     ]
    }
   ],
   "source": [
    "from pprint import pformat # just to print a dictionary nicely\n",
    "from utils import apply_to_all_modules_with_types\n",
    "from sparseprop.utils import sparsity\n",
    "\n",
    "logger.log(\"Sparsity per layer:\")\n",
    "logger.log(pformat(apply_to_all_modules_with_types(\n",
    "    model,\n",
    "    [torch.nn.Linear, torch.nn.Conv2d], # we only want the sparsity of linear and conv2d modules\n",
    "    lambda name, module: f'{sparsity(module):.3f}') # calculate the sparsity for each module\n",
    ", indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is pre-trained on the ImageNet dataset, which consists of 1000 classes. However, for fine-tuning, we will be using the ImageNette dataset, which only has 10 classes. As a result, we will need to replace the classifier layer in order to adapt the model to this specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = torch.nn.Linear(\n",
    "    model.fc.in_features, # number of input features\n",
    "    10, # number of classes in imagenette\n",
    "    bias=model.fc.bias is not None # keep the bias if exists\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get our dataset and dataloaders ready. We directly load the ImageNette dataset from the *SparseML* library. You can run the following commnad to install it on your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sparseml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the library installed, we can load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/scistore14/alistgrp/mnikdan/miniconda3/envs/sp/lib/python3.9/site-packages/sparseml/pytorch/__init__.py:41: UserWarning: sparseml quantized onnx export does not work with torch==1.10.* or 1.11.*\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already downloaded imagenette ImagenetteSize.s320\n",
      "already downloaded imagenette ImagenetteSize.s320\n",
      "Total number of training batches: 51\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sparseml.pytorch.datasets import ImagenetteDataset, ImagenetteSize\n",
    "\n",
    "# load the datasets\n",
    "train_dataset, test_dataset = [ImagenetteDataset(\n",
    "    root='/dev/shm/', # store the dataset in /dev/shm/ to map the dataset to memory and avoid data loading overheads\n",
    "    train=train,\n",
    "    dataset_size=ImagenetteSize.s320,\n",
    "    image_size=224\n",
    ") for train in [True, False]]\n",
    "\n",
    "# prepare the dataloaders\n",
    "train_loader, test_loader = [DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    num_workers=4,\n",
    "    shuffle=True\n",
    ") for dataset in [train_dataset, test_dataset]]\n",
    "\n",
    "logger.log(f'Total number of training batches: {len(train_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's where *SparseProp* comes into play. As explained in the paper, we replace each Linear or Conv2d layer in a network with a sparse one, if the following conditions are met:\n",
    "\n",
    "- It's at least 80% sparse.\n",
    "- The sparse module is faster than the original dense one (in terms of forward+backward time).\n",
    "\n",
    "This behavior is implemented in the `swap_modules_with_sparse` method in `sparseprop.utils`. Let's do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "keeping the module conv1 dense...\n",
      "------------------------------\n",
      "module Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) took 0.6537895202636719 fwd and 1.5996980667114258 bwd\n",
      "module SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=False) took 1.3576796054840088 fwd and 1.9765965938568115 bwd\n",
      "module SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=True) took 0.24248600006103516 fwd and 0.5414636135101318 bwd\n",
      "going with SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=True) with full time of 0.783949613571167\n",
      "module layer1.0.conv1 replaced with SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=True)\n",
      "------------------------------\n",
      "module Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) took 0.6515367031097412 fwd and 1.5962450504302979 bwd\n",
      "module SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=False) took 1.3252687454223633 fwd and 1.9624810218811035 bwd\n",
      "module SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=True) took 0.25075316429138184 fwd and 0.5489640235900879 bwd\n",
      "going with SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=True) with full time of 0.7997171878814697\n",
      "module layer1.0.conv2 replaced with SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=True)\n",
      "------------------------------\n",
      "module Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) took 0.651078462600708 fwd and 1.5953965187072754 bwd\n",
      "module SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=False) took 1.3374743461608887 fwd and 1.9667127132415771 bwd\n",
      "module SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=True) took 0.24969148635864258 fwd and 0.5471739768981934 bwd\n",
      "going with SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=True) with full time of 0.7968654632568359\n",
      "module layer1.1.conv1 replaced with SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=True)\n",
      "------------------------------\n",
      "module Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) took 0.6508922576904297 fwd and 1.5951457023620605 bwd\n",
      "module SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=False) took 1.331139087677002 fwd and 1.9636893272399902 bwd\n",
      "module SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=True) took 0.25159716606140137 fwd and 0.5491302013397217 bwd\n",
      "going with SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=True) with full time of 0.800727367401123\n",
      "module layer1.1.conv2 replaced with SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=True)\n",
      "------------------------------\n",
      "module Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) took 0.36971068382263184 fwd and 0.9490935802459717 bwd\n",
      "module SparseConv2d([128, 64, 3, 3], sp=0.95, nnz=3686, s=2, p=1, voo=False) took 1.1540188789367676 fwd and 0.9293184280395508 bwd\n",
      "module SparseConv2d([128, 64, 3, 3], sp=0.95, nnz=3686, s=2, p=1, voo=True) took 1.1658782958984375 fwd and 0.6493189334869385 bwd\n",
      "going with Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) with full time of 1.3188042640686035\n",
      "keeping the module layer2.0.conv1 dense...\n",
      "------------------------------\n",
      "module Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) took 0.6316447257995605 fwd and 1.5091893672943115 bwd\n",
      "module SparseConv2d([128, 128, 3, 3], sp=0.95, nnz=7373, s=1, p=1, voo=False) took 0.6453986167907715 fwd and 0.9138906002044678 bwd\n",
      "module SparseConv2d([128, 128, 3, 3], sp=0.95, nnz=7373, s=1, p=1, voo=True) took 0.3703117370605469 fwd and 0.687126636505127 bwd\n",
      "going with SparseConv2d([128, 128, 3, 3], sp=0.95, nnz=7373, s=1, p=1, voo=True) with full time of 1.0574383735656738\n",
      "module layer2.0.conv2 replaced with SparseConv2d([128, 128, 3, 3], sp=0.95, nnz=7373, s=1, p=1, voo=True)\n",
      "------------------------------\n",
      "module Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) took 0.0976557731628418 fwd and 0.278552770614624 bwd\n",
      "module SparseConv2d([128, 64, 1, 1], sp=0.95, nnz=410, s=2, p=0, voo=False) took 0.9179229736328125 fwd and 0.7433874607086182 bwd\n",
      "module SparseConv2d([128, 64, 1, 1], sp=0.95, nnz=410, s=2, p=0, voo=True) took 0.9330620765686035 fwd and 0.2980782985687256 bwd\n",
      "going with Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) with full time of 0.3762085437774658\n",
      "keeping the module layer2.0.downsample.0 dense...\n",
      "------------------------------\n",
      "module Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) took 0.6319241523742676 fwd and 1.508660078048706 bwd\n",
      "module SparseConv2d([128, 128, 3, 3], sp=0.95, nnz=7373, s=1, p=1, voo=False) took 0.6373977661132812 fwd and 0.9119634628295898 bwd\n",
      "module SparseConv2d([128, 128, 3, 3], sp=0.95, nnz=7373, s=1, p=1, voo=True) took 0.37430548667907715 fwd and 0.6898000240325928 bwd\n",
      "going with SparseConv2d([128, 128, 3, 3], sp=0.95, nnz=7373, s=1, p=1, voo=True) with full time of 1.06410551071167\n",
      "module layer2.1.conv1 replaced with SparseConv2d([128, 128, 3, 3], sp=0.95, nnz=7373, s=1, p=1, voo=True)\n",
      "------------------------------\n",
      "module Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) took 0.6316280364990234 fwd and 1.5089399814605713 bwd\n",
      "module SparseConv2d([128, 128, 3, 3], sp=0.95, nnz=7373, s=1, p=1, voo=False) took 0.6428439617156982 fwd and 0.9137217998504639 bwd\n",
      "module SparseConv2d([128, 128, 3, 3], sp=0.95, nnz=7373, s=1, p=1, voo=True) took 0.3743324279785156 fwd and 0.6895146369934082 bwd\n",
      "going with SparseConv2d([128, 128, 3, 3], sp=0.95, nnz=7373, s=1, p=1, voo=True) with full time of 1.0638470649719238\n",
      "module layer2.1.conv2 replaced with SparseConv2d([128, 128, 3, 3], sp=0.95, nnz=7373, s=1, p=1, voo=True)\n",
      "------------------------------\n",
      "module Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) took 0.3503093719482422 fwd and 0.8493261337280273 bwd\n",
      "module SparseConv2d([256, 128, 3, 3], sp=0.95, nnz=14746, s=2, p=1, voo=False) took 0.4656362533569336 fwd and 0.5568394660949707 bwd\n",
      "module SparseConv2d([256, 128, 3, 3], sp=0.95, nnz=14746, s=2, p=1, voo=True) took 0.46555352210998535 fwd and 0.8326826095581055 bwd\n",
      "going with SparseConv2d([256, 128, 3, 3], sp=0.95, nnz=14746, s=2, p=1, voo=False) with full time of 1.0224757194519043\n",
      "module layer3.0.conv1 replaced with SparseConv2d([256, 128, 3, 3], sp=0.95, nnz=14746, s=2, p=1, voo=False)\n",
      "------------------------------\n",
      "module Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) took 0.6343159675598145 fwd and 1.3283319473266602 bwd\n",
      "module SparseConv2d([256, 256, 3, 3], sp=0.95, nnz=29491, s=1, p=1, voo=False) took 0.5050003528594971 fwd and 0.6285405158996582 bwd\n",
      "module SparseConv2d([256, 256, 3, 3], sp=0.95, nnz=29491, s=1, p=1, voo=True) took 0.7813265323638916 fwd and 1.2833356857299805 bwd\n",
      "going with SparseConv2d([256, 256, 3, 3], sp=0.95, nnz=29491, s=1, p=1, voo=False) with full time of 1.1335408687591553\n",
      "module layer3.0.conv2 replaced with SparseConv2d([256, 256, 3, 3], sp=0.95, nnz=29491, s=1, p=1, voo=False)\n",
      "------------------------------\n",
      "module Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) took 0.06782793998718262 fwd and 0.1881122589111328 bwd\n",
      "module SparseConv2d([256, 128, 1, 1], sp=0.95, nnz=1638, s=2, p=0, voo=False) took 0.22548270225524902 fwd and 0.385814905166626 bwd\n",
      "module SparseConv2d([256, 128, 1, 1], sp=0.95, nnz=1638, s=2, p=0, voo=True) took 0.22622132301330566 fwd and 0.2331991195678711 bwd\n",
      "going with Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) with full time of 0.25594019889831543\n",
      "keeping the module layer3.0.downsample.0 dense...\n",
      "------------------------------\n",
      "module Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) took 0.6341907978057861 fwd and 1.3260297775268555 bwd\n",
      "module SparseConv2d([256, 256, 3, 3], sp=0.95, nnz=29491, s=1, p=1, voo=False) took 0.49993395805358887 fwd and 0.6333653926849365 bwd\n",
      "module SparseConv2d([256, 256, 3, 3], sp=0.95, nnz=29491, s=1, p=1, voo=True) took 0.7733349800109863 fwd and 1.2643136978149414 bwd\n",
      "going with SparseConv2d([256, 256, 3, 3], sp=0.95, nnz=29491, s=1, p=1, voo=False) with full time of 1.1332993507385254\n",
      "module layer3.1.conv1 replaced with SparseConv2d([256, 256, 3, 3], sp=0.95, nnz=29491, s=1, p=1, voo=False)\n",
      "------------------------------\n",
      "module Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) took 0.6344144344329834 fwd and 1.326756238937378 bwd\n",
      "module SparseConv2d([256, 256, 3, 3], sp=0.95, nnz=29491, s=1, p=1, voo=False) took 0.5124797821044922 fwd and 0.6291441917419434 bwd\n",
      "module SparseConv2d([256, 256, 3, 3], sp=0.95, nnz=29491, s=1, p=1, voo=True) took 0.7771730422973633 fwd and 1.2691457271575928 bwd\n",
      "going with SparseConv2d([256, 256, 3, 3], sp=0.95, nnz=29491, s=1, p=1, voo=False) with full time of 1.1416239738464355\n",
      "module layer3.1.conv2 replaced with SparseConv2d([256, 256, 3, 3], sp=0.95, nnz=29491, s=1, p=1, voo=False)\n",
      "------------------------------\n",
      "module Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) took 0.40129590034484863 fwd and 0.8475296497344971 bwd\n",
      "module SparseConv2d([512, 256, 3, 3], sp=0.95, nnz=58982, s=2, p=1, voo=False) took 0.3875877857208252 fwd and 0.31590890884399414 bwd\n",
      "module SparseConv2d([512, 256, 3, 3], sp=0.95, nnz=58982, s=2, p=1, voo=True) took 0.38722801208496094 fwd and 1.4996874332427979 bwd\n",
      "going with SparseConv2d([512, 256, 3, 3], sp=0.95, nnz=58982, s=2, p=1, voo=False) with full time of 0.7034966945648193\n",
      "module layer4.0.conv1 replaced with SparseConv2d([512, 256, 3, 3], sp=0.95, nnz=58982, s=2, p=1, voo=False)\n",
      "------------------------------\n",
      "module Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) took 0.6806645393371582 fwd and 1.3471698760986328 bwd\n",
      "module SparseConv2d([512, 512, 3, 3], sp=0.95, nnz=117965, s=1, p=1, voo=False) took 0.35182809829711914 fwd and 0.4279053211212158 bwd\n",
      "module SparseConv2d([512, 512, 3, 3], sp=0.95, nnz=117965, s=1, p=1, voo=True) took 1.8099141120910645 fwd and 2.827181339263916 bwd\n",
      "going with SparseConv2d([512, 512, 3, 3], sp=0.95, nnz=117965, s=1, p=1, voo=False) with full time of 0.779733419418335\n",
      "module layer4.0.conv2 replaced with SparseConv2d([512, 512, 3, 3], sp=0.95, nnz=117965, s=1, p=1, voo=False)\n",
      "------------------------------\n",
      "module Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) took 0.04999351501464844 fwd and 0.14213848114013672 bwd\n",
      "module SparseConv2d([512, 256, 1, 1], sp=0.95, nnz=6554, s=2, p=0, voo=False) took 0.12142157554626465 fwd and 0.16385126113891602 bwd\n",
      "module SparseConv2d([512, 256, 1, 1], sp=0.95, nnz=6554, s=2, p=0, voo=True) took 0.1244204044342041 fwd and 0.3151395320892334 bwd\n",
      "going with Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) with full time of 0.19213199615478516\n",
      "keeping the module layer4.0.downsample.0 dense...\n",
      "------------------------------\n",
      "module Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) took 0.6791129112243652 fwd and 1.341883659362793 bwd\n",
      "module SparseConv2d([512, 512, 3, 3], sp=0.95, nnz=117965, s=1, p=1, voo=False) took 0.334489107131958 fwd and 0.41767215728759766 bwd\n",
      "module SparseConv2d([512, 512, 3, 3], sp=0.95, nnz=117965, s=1, p=1, voo=True) took 1.7890021800994873 fwd and 2.7480850219726562 bwd\n",
      "going with SparseConv2d([512, 512, 3, 3], sp=0.95, nnz=117965, s=1, p=1, voo=False) with full time of 0.7521612644195557\n",
      "module layer4.1.conv1 replaced with SparseConv2d([512, 512, 3, 3], sp=0.95, nnz=117965, s=1, p=1, voo=False)\n",
      "------------------------------\n",
      "module Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) took 0.6805310249328613 fwd and 1.3308117389678955 bwd\n",
      "module SparseConv2d([512, 512, 3, 3], sp=0.95, nnz=117965, s=1, p=1, voo=False) took 0.3323853015899658 fwd and 0.41096043586730957 bwd\n",
      "module SparseConv2d([512, 512, 3, 3], sp=0.95, nnz=117965, s=1, p=1, voo=True) took 1.7962658405303955 fwd and 2.7770447731018066 bwd\n",
      "going with SparseConv2d([512, 512, 3, 3], sp=0.95, nnz=117965, s=1, p=1, voo=False) with full time of 0.7433457374572754\n",
      "module layer4.1.conv2 replaced with SparseConv2d([512, 512, 3, 3], sp=0.95, nnz=117965, s=1, p=1, voo=False)\n",
      "------------------------------\n",
      "keeping the module fc dense...\n"
     ]
    }
   ],
   "source": [
    "from sparseprop.utils import swap_modules_with_sparse\n",
    "\n",
    "input_shape = next(iter(train_loader))[0].shape # we need the shape of our data\n",
    "\n",
    "# here's where the magic happens\n",
    "model = swap_modules_with_sparse(model, input_shape, inplace=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=True)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=True)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=True)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): SparseConv2d([64, 64, 3, 3], sp=0.95, nnz=1843, s=1, p=1, voo=True)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): SparseConv2d([128, 128, 3, 3], sp=0.95, nnz=7373, s=1, p=1, voo=True)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): SparseConv2d([128, 128, 3, 3], sp=0.95, nnz=7373, s=1, p=1, voo=True)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): SparseConv2d([128, 128, 3, 3], sp=0.95, nnz=7373, s=1, p=1, voo=True)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): SparseConv2d([256, 128, 3, 3], sp=0.95, nnz=14746, s=2, p=1, voo=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): SparseConv2d([256, 256, 3, 3], sp=0.95, nnz=29491, s=1, p=1, voo=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): SparseConv2d([256, 256, 3, 3], sp=0.95, nnz=29491, s=1, p=1, voo=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): SparseConv2d([256, 256, 3, 3], sp=0.95, nnz=29491, s=1, p=1, voo=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): SparseConv2d([512, 256, 3, 3], sp=0.95, nnz=58982, s=2, p=1, voo=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): SparseConv2d([512, 512, 3, 3], sp=0.95, nnz=117965, s=1, p=1, voo=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): SparseConv2d([512, 512, 3, 3], sp=0.95, nnz=117965, s=1, p=1, voo=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): SparseConv2d([512, 512, 3, 3], sp=0.95, nnz=117965, s=1, p=1, voo=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model and dataloaders ready, let's create our loss criterion and optimizer objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optim\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-3, momentum=0.9, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create our `Finetuner` object to perform one epoch of training. This object handles training and validaton, as well as performing extensive timings on the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1, Step 1: loss=2.7053, acc=0.0820\n",
      "Timings: avg_end_to_end_forward=11.1022, avg_end_to_end_backward=11.5243, avg_end_to_end_minibatch=22.6428, avg_module_forward_sum=6.9712, avg_module_backward_sum=7.5827\n",
      "[Train] Epoch 1, Step 2: loss=2.6545, acc=0.0898\n",
      "Timings: avg_end_to_end_forward=11.0292, avg_end_to_end_backward=11.3527, avg_end_to_end_minibatch=22.3985, avg_module_forward_sum=6.9279, avg_module_backward_sum=7.5711\n",
      "[Train] Epoch 1, Step 3: loss=2.5691, acc=0.0924\n",
      "Timings: avg_end_to_end_forward=11.0056, avg_end_to_end_backward=11.2854, avg_end_to_end_minibatch=22.3066, avg_module_forward_sum=6.9138, avg_module_backward_sum=7.5587\n",
      "[Train] Epoch 1, Step 4: loss=2.4842, acc=0.1113\n",
      "Timings: avg_end_to_end_forward=10.9948, avg_end_to_end_backward=11.2466, avg_end_to_end_minibatch=22.2568, avg_module_forward_sum=6.9054, avg_module_backward_sum=7.5530\n",
      "[Train] Epoch 1, Step 5: loss=2.3990, acc=0.1437\n",
      "Timings: avg_end_to_end_forward=10.9984, avg_end_to_end_backward=11.2293, avg_end_to_end_minibatch=22.2431, avg_module_forward_sum=6.9095, avg_module_backward_sum=7.5502\n",
      "[Train] Epoch 1, Step 6: loss=2.3070, acc=0.1953\n",
      "Timings: avg_end_to_end_forward=10.9899, avg_end_to_end_backward=11.2169, avg_end_to_end_minibatch=22.2220, avg_module_forward_sum=6.9045, avg_module_backward_sum=7.5499\n",
      "[Train] Epoch 1, Step 7: loss=2.2150, acc=0.2394\n",
      "Timings: avg_end_to_end_forward=10.9836, avg_end_to_end_backward=11.1999, avg_end_to_end_minibatch=22.1986, avg_module_forward_sum=6.9007, avg_module_backward_sum=7.5457\n",
      "[Train] Epoch 1, Step 8: loss=2.1194, acc=0.2910\n",
      "Timings: avg_end_to_end_forward=10.9775, avg_end_to_end_backward=11.1892, avg_end_to_end_minibatch=22.1819, avg_module_forward_sum=6.8955, avg_module_backward_sum=7.5437\n",
      "[Train] Epoch 1, Step 9: loss=2.0242, acc=0.3368\n",
      "Timings: avg_end_to_end_forward=10.9733, avg_end_to_end_backward=11.1817, avg_end_to_end_minibatch=22.1701, avg_module_forward_sum=6.8918, avg_module_backward_sum=7.5428\n",
      "[Train] Epoch 1, Step 10: loss=1.9279, acc=0.3820\n",
      "Timings: avg_end_to_end_forward=10.9689, avg_end_to_end_backward=11.1748, avg_end_to_end_minibatch=22.1587, avg_module_forward_sum=6.8890, avg_module_backward_sum=7.5417\n",
      "[Train] Epoch 1, Step 11: loss=1.8320, acc=0.4244\n",
      "Timings: avg_end_to_end_forward=10.9660, avg_end_to_end_backward=11.1696, avg_end_to_end_minibatch=22.1503, avg_module_forward_sum=6.8869, avg_module_backward_sum=7.5417\n",
      "[Train] Epoch 1, Step 12: loss=1.7435, acc=0.4622\n",
      "Timings: avg_end_to_end_forward=10.9614, avg_end_to_end_backward=11.1651, avg_end_to_end_minibatch=22.1414, avg_module_forward_sum=6.8831, avg_module_backward_sum=7.5407\n",
      "[Train] Epoch 1, Step 13: loss=1.6589, acc=0.4964\n",
      "Timings: avg_end_to_end_forward=10.9593, avg_end_to_end_backward=11.1597, avg_end_to_end_minibatch=22.1338, avg_module_forward_sum=6.8812, avg_module_backward_sum=7.5393\n",
      "[Train] Epoch 1, Step 14: loss=1.5815, acc=0.5232\n",
      "Timings: avg_end_to_end_forward=10.9577, avg_end_to_end_backward=11.1564, avg_end_to_end_minibatch=22.1289, avg_module_forward_sum=6.8801, avg_module_backward_sum=7.5387\n",
      "[Train] Epoch 1, Step 15: loss=1.5068, acc=0.5482\n",
      "Timings: avg_end_to_end_forward=10.9548, avg_end_to_end_backward=11.1531, avg_end_to_end_minibatch=22.1226, avg_module_forward_sum=6.8784, avg_module_backward_sum=7.5377\n",
      "[Train] Epoch 1, Step 16: loss=1.4409, acc=0.5708\n",
      "Timings: avg_end_to_end_forward=10.9528, avg_end_to_end_backward=11.1505, avg_end_to_end_minibatch=22.1179, avg_module_forward_sum=6.8770, avg_module_backward_sum=7.5373\n",
      "[Train] Epoch 1, Step 17: loss=1.3755, acc=0.5926\n",
      "Timings: avg_end_to_end_forward=10.9501, avg_end_to_end_backward=11.1484, avg_end_to_end_minibatch=22.1131, avg_module_forward_sum=6.8749, avg_module_backward_sum=7.5369\n",
      "[Train] Epoch 1, Step 18: loss=1.3169, acc=0.6122\n",
      "Timings: avg_end_to_end_forward=10.9485, avg_end_to_end_backward=11.1465, avg_end_to_end_minibatch=22.1097, avg_module_forward_sum=6.8733, avg_module_backward_sum=7.5366\n",
      "[Train] Epoch 1, Step 19: loss=1.2669, acc=0.6283\n",
      "Timings: avg_end_to_end_forward=10.9475, avg_end_to_end_backward=11.1445, avg_end_to_end_minibatch=22.1066, avg_module_forward_sum=6.8717, avg_module_backward_sum=7.5362\n",
      "[Train] Epoch 1, Step 20: loss=1.2189, acc=0.6426\n",
      "Timings: avg_end_to_end_forward=10.9462, avg_end_to_end_backward=11.1480, avg_end_to_end_minibatch=22.1088, avg_module_forward_sum=6.8707, avg_module_backward_sum=7.5408\n",
      "[Train] Epoch 1, Step 21: loss=1.1756, acc=0.6561\n",
      "Timings: avg_end_to_end_forward=10.9446, avg_end_to_end_backward=11.1459, avg_end_to_end_minibatch=22.1052, avg_module_forward_sum=6.8697, avg_module_backward_sum=7.5399\n",
      "[Train] Epoch 1, Step 22: loss=1.1365, acc=0.6674\n",
      "Timings: avg_end_to_end_forward=10.9437, avg_end_to_end_backward=11.1436, avg_end_to_end_minibatch=22.1018, avg_module_forward_sum=6.8690, avg_module_backward_sum=7.5391\n",
      "[Train] Epoch 1, Step 23: loss=1.0966, acc=0.6795\n",
      "Timings: avg_end_to_end_forward=10.9420, avg_end_to_end_backward=11.1421, avg_end_to_end_minibatch=22.0986, avg_module_forward_sum=6.8679, avg_module_backward_sum=7.5384\n",
      "[Train] Epoch 1, Step 24: loss=1.0588, acc=0.6911\n",
      "Timings: avg_end_to_end_forward=10.9409, avg_end_to_end_backward=11.1407, avg_end_to_end_minibatch=22.0961, avg_module_forward_sum=6.8672, avg_module_backward_sum=7.5378\n",
      "[Train] Epoch 1, Step 25: loss=1.0273, acc=0.7005\n",
      "Timings: avg_end_to_end_forward=10.9401, avg_end_to_end_backward=11.1393, avg_end_to_end_minibatch=22.0939, avg_module_forward_sum=6.8662, avg_module_backward_sum=7.5373\n",
      "[Train] Epoch 1, Step 26: loss=0.9959, acc=0.7099\n",
      "Timings: avg_end_to_end_forward=10.9389, avg_end_to_end_backward=11.1386, avg_end_to_end_minibatch=22.0920, avg_module_forward_sum=6.8654, avg_module_backward_sum=7.5372\n",
      "[Train] Epoch 1, Step 27: loss=0.9684, acc=0.7177\n",
      "Timings: avg_end_to_end_forward=10.9377, avg_end_to_end_backward=11.1385, avg_end_to_end_minibatch=22.0907, avg_module_forward_sum=6.8644, avg_module_backward_sum=7.5375\n",
      "[Train] Epoch 1, Step 28: loss=0.9410, acc=0.7257\n",
      "Timings: avg_end_to_end_forward=10.9364, avg_end_to_end_backward=11.1374, avg_end_to_end_minibatch=22.0883, avg_module_forward_sum=6.8635, avg_module_backward_sum=7.5369\n",
      "[Train] Epoch 1, Step 29: loss=0.9145, acc=0.7337\n",
      "Timings: avg_end_to_end_forward=10.9347, avg_end_to_end_backward=11.1365, avg_end_to_end_minibatch=22.0857, avg_module_forward_sum=6.8626, avg_module_backward_sum=7.5365\n",
      "[Train] Epoch 1, Step 30: loss=0.8888, acc=0.7417\n",
      "Timings: avg_end_to_end_forward=10.9340, avg_end_to_end_backward=11.1346, avg_end_to_end_minibatch=22.0831, avg_module_forward_sum=6.8622, avg_module_backward_sum=7.5355\n",
      "[Train] Epoch 1, Step 31: loss=0.8657, acc=0.7485\n",
      "Timings: avg_end_to_end_forward=10.9326, avg_end_to_end_backward=11.1333, avg_end_to_end_minibatch=22.0804, avg_module_forward_sum=6.8615, avg_module_backward_sum=7.5347\n",
      "[Train] Epoch 1, Step 32: loss=0.8440, acc=0.7551\n",
      "Timings: avg_end_to_end_forward=10.9313, avg_end_to_end_backward=11.1323, avg_end_to_end_minibatch=22.0780, avg_module_forward_sum=6.8609, avg_module_backward_sum=7.5343\n",
      "[Train] Epoch 1, Step 33: loss=0.8252, acc=0.7603\n",
      "Timings: avg_end_to_end_forward=10.9299, avg_end_to_end_backward=11.1316, avg_end_to_end_minibatch=22.0759, avg_module_forward_sum=6.8602, avg_module_backward_sum=7.5339\n",
      "[Train] Epoch 1, Step 34: loss=0.8085, acc=0.7652\n",
      "Timings: avg_end_to_end_forward=10.9294, avg_end_to_end_backward=11.1306, avg_end_to_end_minibatch=22.0744, avg_module_forward_sum=6.8596, avg_module_backward_sum=7.5335\n",
      "[Train] Epoch 1, Step 35: loss=0.7913, acc=0.7702\n",
      "Timings: avg_end_to_end_forward=10.9282, avg_end_to_end_backward=11.1299, avg_end_to_end_minibatch=22.0725, avg_module_forward_sum=6.8589, avg_module_backward_sum=7.5331\n",
      "[Train] Epoch 1, Step 36: loss=0.7739, acc=0.7753\n",
      "Timings: avg_end_to_end_forward=10.9274, avg_end_to_end_backward=11.1286, avg_end_to_end_minibatch=22.0704, avg_module_forward_sum=6.8586, avg_module_backward_sum=7.5325\n",
      "[Train] Epoch 1, Step 37: loss=0.7571, acc=0.7798\n",
      "Timings: avg_end_to_end_forward=10.9265, avg_end_to_end_backward=11.1282, avg_end_to_end_minibatch=22.0691, avg_module_forward_sum=6.8579, avg_module_backward_sum=7.5323\n",
      "[Train] Epoch 1, Step 38: loss=0.7410, acc=0.7843\n",
      "Timings: avg_end_to_end_forward=10.9260, avg_end_to_end_backward=11.1273, avg_end_to_end_minibatch=22.0676, avg_module_forward_sum=6.8575, avg_module_backward_sum=7.5317\n",
      "[Train] Epoch 1, Step 39: loss=0.7258, acc=0.7887\n",
      "Timings: avg_end_to_end_forward=10.9254, avg_end_to_end_backward=11.1263, avg_end_to_end_minibatch=22.0661, avg_module_forward_sum=6.8570, avg_module_backward_sum=7.5312\n",
      "[Train] Epoch 1, Step 40: loss=0.7112, acc=0.7929\n",
      "Timings: avg_end_to_end_forward=10.9251, avg_end_to_end_backward=11.1261, avg_end_to_end_minibatch=22.0656, avg_module_forward_sum=6.8568, avg_module_backward_sum=7.5313\n",
      "[Train] Epoch 1, Step 41: loss=0.6990, acc=0.7965\n",
      "Timings: avg_end_to_end_forward=10.9247, avg_end_to_end_backward=11.1253, avg_end_to_end_minibatch=22.0643, avg_module_forward_sum=6.8564, avg_module_backward_sum=7.5309\n",
      "[Train] Epoch 1, Step 42: loss=0.6863, acc=0.7999\n",
      "Timings: avg_end_to_end_forward=10.9238, avg_end_to_end_backward=11.1250, avg_end_to_end_minibatch=22.0632, avg_module_forward_sum=6.8558, avg_module_backward_sum=7.5310\n",
      "[Train] Epoch 1, Step 43: loss=0.6744, acc=0.8031\n",
      "Timings: avg_end_to_end_forward=10.9233, avg_end_to_end_backward=11.1247, avg_end_to_end_minibatch=22.0624, avg_module_forward_sum=6.8555, avg_module_backward_sum=7.5309\n",
      "[Train] Epoch 1, Step 44: loss=0.6626, acc=0.8068\n",
      "Timings: avg_end_to_end_forward=10.9226, avg_end_to_end_backward=11.1241, avg_end_to_end_minibatch=22.0611, avg_module_forward_sum=6.8550, avg_module_backward_sum=7.5306\n",
      "[Train] Epoch 1, Step 45: loss=0.6506, acc=0.8105\n",
      "Timings: avg_end_to_end_forward=10.9218, avg_end_to_end_backward=11.1239, avg_end_to_end_minibatch=22.0601, avg_module_forward_sum=6.8546, avg_module_backward_sum=7.5305\n",
      "[Train] Epoch 1, Step 46: loss=0.6392, acc=0.8139\n",
      "Timings: avg_end_to_end_forward=10.9208, avg_end_to_end_backward=11.1235, avg_end_to_end_minibatch=22.0587, avg_module_forward_sum=6.8541, avg_module_backward_sum=7.5305\n",
      "[Train] Epoch 1, Step 47: loss=0.6296, acc=0.8167\n",
      "Timings: avg_end_to_end_forward=10.9201, avg_end_to_end_backward=11.1235, avg_end_to_end_minibatch=22.0580, avg_module_forward_sum=6.8537, avg_module_backward_sum=7.5307\n",
      "[Train] Epoch 1, Step 48: loss=0.6197, acc=0.8195\n",
      "Timings: avg_end_to_end_forward=10.9199, avg_end_to_end_backward=11.1232, avg_end_to_end_minibatch=22.0574, avg_module_forward_sum=6.8537, avg_module_backward_sum=7.5307\n",
      "[Train] Epoch 1, Step 49: loss=0.6094, acc=0.8224\n",
      "Timings: avg_end_to_end_forward=10.9195, avg_end_to_end_backward=11.1230, avg_end_to_end_minibatch=22.0568, avg_module_forward_sum=6.8534, avg_module_backward_sum=7.5306\n",
      "[Train] Epoch 1, Step 50: loss=0.6001, acc=0.8251\n",
      "Timings: avg_end_to_end_forward=10.9193, avg_end_to_end_backward=11.1227, avg_end_to_end_minibatch=22.0564, avg_module_forward_sum=6.8533, avg_module_backward_sum=7.5306\n",
      "[Train] Epoch 1, Step 51: loss=0.5944, acc=0.8266\n",
      "Timings: avg_end_to_end_forward=10.7752, avg_end_to_end_backward=10.9864, avg_end_to_end_minibatch=21.7758, avg_module_forward_sum=6.7629, avg_module_backward_sum=7.4425\n",
      "[Train] Epoch 1: loss=0.5944, acc=0.8266\n",
      "Timings: avg_end_to_end_forward=10.7752, avg_end_to_end_backward=10.9864, avg_end_to_end_minibatch=21.7758, avg_module_forward_sum=6.7629, avg_module_backward_sum=7.4425\n",
      "Epoch 1 training took 1112.7971.\n",
      "[Val] Epoch 1: loss=0.1265, acc=0.9638\n",
      "Timings: avg_end_to_end_forward=9.7825, avg_end_to_end_backward=0.0000, avg_end_to_end_minibatch=9.7828, avg_module_forward_sum=6.9401, avg_module_backward_sum=0.0000\n",
      "Epoch 1 validation took 20.6065.\n",
      "The full finetuning took 1133.4627.\n"
     ]
    }
   ],
   "source": [
    "from utils import Finetuner\n",
    "\n",
    "# initialize the finetuner\n",
    "finetuner = Finetuner(\n",
    "    model,\n",
    "    optimizer,\n",
    "    schedular=None, # we could pass an lr schedular here. no need for this example.\n",
    "    loss_fn=loss_fn,\n",
    "    log_freq=1, # how often to log (in batches). 1 means that it will log after processing every batch.\n",
    "    save_freq=1, # how often to save the checkpoint (in epochs). 1 means that it will save a checkpoint after each epoch.\n",
    "    logger=logger\n",
    ").finetune(train_loader, test_loader, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in addition to the loss and accuracy metrics, this script also reports the time spent in each part of the process. The timings include:\n",
    "\n",
    "- `avg_end_to_end_forward`: the average time spent in the forward pass, i.e., the model(inputs) line.\n",
    "- `avg_end_to_end_backward`: the average time spent in the backward pass, i.e., the loss.backward() line.\n",
    "- `avg_end_to_end_minibatch`: the average time spent processing a minibatch. This includes forward pass, backward pass, loss calculation, optimization step, etc. Note that loading the data into memory is not included.\n",
    "- `avg_module_forward_sum`: the average time spent in the forward function of the modules torch.nn.Linear, torch.nn.Conv2d, SparseLinear, and SparseConv2d.\n",
    "- `avg_module_backward_sum`: the average time spent in the backward function of the modules torch.nn.Linear, torch.nn.Conv2d, SparseLinear, and SparseConv2d.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we provided step-by-step examples of how to benefit from *SparseProp*'s sparse implementation for speeding-up a single layer, as well as an entire network. For the latter, we took a 95% uniform sparse ResNet18 model (pretrained on ImageNet), and fine-tuned it on the ImageNette dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sp)",
   "language": "python",
   "name": "sp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
